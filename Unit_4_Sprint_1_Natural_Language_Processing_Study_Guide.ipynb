{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unit_4_Sprint_1_Natural_Language_Processing_Study_Guide.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enodNfbMIxzN",
        "colab_type": "text"
      },
      "source": [
        "This study guide should reinforce and provide practice for all of the concepts you have seen in the past week. There are a mix of written questions and coding exercises, both are equally important to prepare you for the sprint challenge as well as to be able to speak on these topics comfortably in interviews and on the job.\n",
        "\n",
        "If you get stuck or are unsure of something remember the 20 minute rule. If that doesn't help, then research a solution with google and stackoverflow. Only once you have exausted these methods should you turn to your Team Lead - they won't be there on your SC or during an interview. That being said, don't hesitate to ask for help if you truly are stuck.\n",
        "\n",
        "Have fun studying!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raVkydRcReX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18517855-ee80-4168-bf7d-7aaf24864ebc"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 151kB/s \n",
            "\u001b[?25hCollecting pyLDAvis==2.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 48.5MB/s \n",
            "\u001b[?25hCollecting spacy==2.2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/47/13/80ad28ef7a16e2a86d16d73e28588be5f1085afd3e85e4b9b912bd700e8a/spacy-2.2.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4MB 43.2MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/7f/366dcba1ba076a88a50bea732dbc033c0c5bbf7876010e6edc67948579d5/scikit_learn-0.22.2-cp36-cp36m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 25.1MB/s \n",
            "\u001b[?25hCollecting seaborn==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/76/220ba4420459d9c4c9c9587c6ce607bf56c25b3d3d2de62056efe482dadc/seaborn-0.9.0-py3-none-any.whl (208kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 50.3MB/s \n",
            "\u001b[?25hCollecting squarify==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0b/2b/2e77c35326efec19819cd1d729540d4d235e6c2a3f37658288a363a67da5/squarify-0.4.3-py3-none-any.whl\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (4.6.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.34.2)\n",
            "Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.15.1)\n",
            "Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (2.7.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (3.6.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.16.0)\n",
            "Collecting funcy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (47.1.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (0.6.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Collecting thinc<7.4.0,>=7.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/59/6bb553bc9a5f072d3cd479fc939fea0f6f682892f1f5cff98de5c9b615bb/thinc-7.3.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 44.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (2.0.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.2.3->-r requirements.txt (line 3)) (1.0.2)\n",
            "Requirement already satisfied: matplotlib>=1.4.3 in /usr/local/lib/python3.6/dist-packages (from seaborn==0.9.0->-r requirements.txt (line 5)) (3.2.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (4.5.3)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r requirements.txt (line 7)) (5.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (1.13.19)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (2.49.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (1.8.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (8.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis==2.1.2->-r requirements.txt (line 2)) (19.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3->-r requirements.txt (line 3)) (2.9)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (1.6.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3->-r requirements.txt (line 3)) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.4.3->seaborn==0.9.0->-r requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (2.1.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r requirements.txt (line 7)) (19.0.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.19 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (1.16.19)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy==2.2.3->-r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.19->boto3->smart-open>=1.8.1->gensim==3.8.1->-r requirements.txt (line 1)) (0.15.2)\n",
            "Building wheels for collected packages: pyLDAvis, funcy\n",
            "  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=d713ae181a66ee8fc844f4802df2ebd367dda9d37db7c933be0412cdb1e4cf16\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n",
            "  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=245f48c9168a400c32244b7a585b47a95ca86f38cd019fced32b762df078057d\n",
            "  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n",
            "Successfully built pyLDAvis funcy\n",
            "Installing collected packages: gensim, funcy, pyLDAvis, thinc, spacy, scikit-learn, seaborn, squarify\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Found existing installation: seaborn 0.10.1\n",
            "    Uninstalling seaborn-0.10.1:\n",
            "      Successfully uninstalled seaborn-0.10.1\n",
            "Successfully installed funcy-1.14 gensim-3.8.1 pyLDAvis-2.1.2 scikit-learn-0.22.2 seaborn-0.9.0 spacy-2.2.3 squarify-0.4.3 thinc-7.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "spacy",
                  "thinc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K69yNifZTLFQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "6014a01a-b060-4138-b0dd-f65bc94e0afc"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_lg==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n",
            "\u001b[K     |████████████████████████████████| 827.9MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (47.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-lg\n",
            "  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=15a77d4d4fc10acfaa809b367184e7a6fbd1990869b21375ed89736f23184bf7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1g7wcyh1/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n",
            "Successfully built en-core-web-lg\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjVNoILlDD83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQBp_ddC9CX",
        "colab_type": "code",
        "outputId": "6f1a14d7-4c74-4d7b-9aed-94d7b5eb1609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/bundickm/Study-Guides/master/data/cannabis.csv')\n",
        "print('Shape:', df.shape, '\\n')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: (2351, 6) \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Strain</th>\n",
              "      <th>Type</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Effects</th>\n",
              "      <th>Flavor</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100-Og</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.0</td>\n",
              "      <td>Creative,Energetic,Tingly,Euphoric,Relaxed</td>\n",
              "      <td>Earthy,Sweet,Citrus</td>\n",
              "      <td>$100 OG is a 50/50 hybrid strain that packs a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98-White-Widow</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.7</td>\n",
              "      <td>Relaxed,Aroused,Creative,Happy,Energetic</td>\n",
              "      <td>Flowery,Violet,Diesel</td>\n",
              "      <td>The ‘98 Aloha White Widow is an especially pot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1024</td>\n",
              "      <td>sativa</td>\n",
              "      <td>4.4</td>\n",
              "      <td>Uplifted,Happy,Relaxed,Energetic,Creative</td>\n",
              "      <td>Spicy/Herbal,Sage,Woody</td>\n",
              "      <td>1024 is a sativa-dominant hybrid bred in Spain...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13-Dawgs</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.2</td>\n",
              "      <td>Tingly,Creative,Hungry,Relaxed,Uplifted</td>\n",
              "      <td>Apricot,Citrus,Grapefruit</td>\n",
              "      <td>13 Dawgs is a hybrid of G13 and Chemdawg genet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24K-Gold</td>\n",
              "      <td>hybrid</td>\n",
              "      <td>4.6</td>\n",
              "      <td>Happy,Relaxed,Euphoric,Uplifted,Talkative</td>\n",
              "      <td>Citrus,Earthy,Orange</td>\n",
              "      <td>Also known as Kosher Tangie, 24k Gold is a 60%...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Strain  ...                                        Description\n",
              "0          100-Og  ...  $100 OG is a 50/50 hybrid strain that packs a ...\n",
              "1  98-White-Widow  ...  The ‘98 Aloha White Widow is an especially pot...\n",
              "2            1024  ...  1024 is a sativa-dominant hybrid bred in Spain...\n",
              "3        13-Dawgs  ...  13 Dawgs is a hybrid of G13 and Chemdawg genet...\n",
              "4        24K-Gold  ...  Also known as Kosher Tangie, 24k Gold is a 60%...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbpf-sf-DjRi",
        "colab_type": "text"
      },
      "source": [
        "# Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8pe3aGUJUkI",
        "colab_type": "text"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3GHPBZ4I3h5",
        "colab_type": "text"
      },
      "source": [
        "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
        "<br/><br/>\n",
        "\n",
        "**Natural Language Processing (NLP)**: `The branch of Data Science which deals with engineering human language features into a form which can be analyzed statistically.`\n",
        "\n",
        "**Token**: `The portion of a string which lies between white space.`\n",
        "\n",
        "**Corpus**: `Your Answer Here`\n",
        "\n",
        "**Stopwords**: `Which which do not contribute sufficient insight in statistical analyses of a text or texts.`\n",
        "\n",
        "**Statistical Trimming**: `A method wherein words which appear with a rather low or high frequency are removed from the data prior to analysis.`\n",
        "\n",
        "**Stemming**: `An method wherein words which share a common base such as ski, skis, and skiing are, grouped all counted under their base form, in this case ski. This method is designed to eliminate duplicates of the same concept and ensure the best sampling of words.`\n",
        "\n",
        "**Lemmatization**: `Lemmatization operates on the same principle as stemming but in a more advanced way. Lemmatization is capable of dealing with stems which diverge such as wolf and wolves, which stemming would mistakenly divide in to wolf and wolv. Lemmatization accomplishes this by breaking words down to their base grammatical forms, e.g. converting \"is\" to \"[to] be\"`\n",
        "\n",
        "**Vectorization**: `Vectorization converts text to matrices which a Data Scientist can then use statistical methods to glean insights from.`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbXlWbA3JWuU",
        "colab_type": "text"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjm-Ab4sJaOs",
        "colab_type": "text"
      },
      "source": [
        "1. What are at least 4 common cleaning tasks you need to do when creating tokens?\n",
        " 1. `If webscraping, remove HTML or other extraneous elements, or convert them to text that Python can read.`\n",
        " 2. `Perform stemming or lemmatization to group duplicates of the same base word together.`\n",
        " 3. `Remove stop words, add custom stop words as needed.`\n",
        " 4. `Account for named entities and other appropriate n-grams.`\n",
        "\n",
        "2. Why is it important to apply custom stopwords to our dataset in addition to the ones that come in a library like spaCy?\n",
        "```\n",
        "Because when examining text from a certain category, i.e. movie reviews, certain words will occur with such a high frequency that they do not contribute substantially to an analysis of what makes each text different. In our movie example, the words movie and film would likely be appropriate custom stop words given that their presence would not be differentiable between entries.\n",
        "```\n",
        "\n",
        "3. Explain the tradeoffs between statistical trimming, stemming, and lemmatizing.\n",
        "```\n",
        "Stemming maximizes for recall; whereas, lemmatizing maximizes for precision.\n",
        "```\n",
        "\n",
        "4. Why do we need to vectorize our documents?\n",
        "```\n",
        "So that the computer can perform statistics on them.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn3Z587YMwnE",
        "colab_type": "text"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii_LYpeoDrTp",
        "colab_type": "text"
      },
      "source": [
        "Write a function to tokenize the `Description` column. Make sure to include the following:\n",
        "- Return the tokens in an iterable structure\n",
        "- Normalize the case\n",
        "- Remove non-alphanumeric characters such as punctuation, whitespace, unicode, etc.\n",
        "- Apply stopwords and make sure to add stopwords specific to this dataset\n",
        "- Lemmatize the tokens before returning them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgt9aT-TDFcq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "2daa6f97-3f7f-4f03-bec5-1b430908ec53"
      },
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "\n",
        "def tokenize(text):\n",
        "\n",
        "  tokens = []\n",
        "\n",
        "  for doc in text:\n",
        "    doc = nlp(text)\n",
        "    tokens.append(doc)\n",
        "    \n",
        "  return tokens"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0309969d9a8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"en_core_web_lg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_lg'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX8ZRE7fFqcw",
        "colab_type": "text"
      },
      "source": [
        "Apply your function to `Description` and save the resulting tokens in a new column, `Tokens`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9pQJYfZFxd1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Tokens\"] = df[\"Description\"].apply(tokenize)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4x9xuBVF4Nh",
        "colab_type": "text"
      },
      "source": [
        "Use the function below to create a `word_count` dataframe based off the `df['Tokens']` column you created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zu2dfbcGz2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count(docs):\n",
        "        word_counts = Counter()\n",
        "        appears_in = Counter()\n",
        "        total_docs = len(docs)\n",
        "\n",
        "        for doc in docs:\n",
        "            word_counts.update(doc)\n",
        "            appears_in.update(set(doc))\n",
        "\n",
        "        temp = zip(word_counts.keys(), word_counts.values())\n",
        "        wc = pd.DataFrame(temp, columns = ['word', 'count'])\n",
        "\n",
        "        wc['rank'] = wc['count'].rank(method='first', ascending=False)\n",
        "        total = wc['count'].sum()\n",
        "\n",
        "        wc['pct_total'] = wc['count'].apply(lambda x: x / total)\n",
        "        \n",
        "        wc = wc.sort_values(by='rank')\n",
        "        wc['cul_pct_total'] = wc['pct_total'].cumsum()\n",
        "\n",
        "        t2 = zip(appears_in.keys(), appears_in.values())\n",
        "        ac = pd.DataFrame(t2, columns=['word', 'appears_in'])\n",
        "        wc = ac.merge(wc, on='word')\n",
        "\n",
        "        wc['appears_in_pct'] = wc['appears_in'].apply(lambda x: x / total_docs)\n",
        "        \n",
        "        return wc.sort_values(by='rank')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94lL-w_uGzzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPPxVzGIHUF8",
        "colab_type": "text"
      },
      "source": [
        "Run the line of code below, and then explain how to interpret the graph.\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWqbuy68Ib0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.lineplot(x='rank', y='cul_pct_total', data=word_count);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-_e03NrMjIO",
        "colab_type": "text"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tQRlWI7UM4ah"
      },
      "source": [
        "## Definitions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djODVPGjM4ao"
      },
      "source": [
        "Define the following terms in your own words, do not simply copy and paste a definition found elsewhere but reword it to be understandable and memorable to you. *Double click the markdown to add your definitions.*\n",
        "<br/><br/>\n",
        "\n",
        "**Vectorization**: `Your Answer Here`\n",
        "\n",
        "**Document Term Matrix (DTM)**: `Your Answer Here`\n",
        "\n",
        "**Latent Semantic Analysis**: `Your Answer Here`\n",
        "\n",
        "**Term Frequency - Inverse Document Frequency (TF-IDF)**: `Your Answer Here`\n",
        "\n",
        "**Word Embedding**: `Your Answer Here`\n",
        "\n",
        "**N-Gram**: `Your Answer Here`\n",
        "\n",
        "**Skip-Gram**: `Your Answer Here`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lOsi6xE4M-cS"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3_Atsw1bM-cY"
      },
      "source": [
        "1. Why do we need to vectorize our documents?\n",
        "```\n",
        "Your Answer Here\n",
        "```\n",
        "\n",
        "2. How is TF-IDF different from simple word frequency? Why do we use TF-IDF over word frequency?\n",
        "```\n",
        "Your Answer Here\n",
        "```\n",
        "\n",
        "3. Why might we choose a word embedding approach over a bag-of-words approach when it comes to vectorization?\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SogHDgfhMTsc",
        "colab_type": "text"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7QrjSwIMYzB",
        "colab_type": "text"
      },
      "source": [
        "Use the dataframe `df` above to complete the following."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BTQbHxIMeQN",
        "colab_type": "text"
      },
      "source": [
        "Vectorize the `Tokens` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka0AywjNMBMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B26eq5wKMrF4",
        "colab_type": "text"
      },
      "source": [
        "Build a Nearest Neighbors model from your dataframe and then find the 5 nearest neighbors to the strain \"100-OG\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcwURJatMp7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvGLfBxDW6D7",
        "colab_type": "text"
      },
      "source": [
        "You will be putting together a classification model below, but before you do you'll need a baseline. Run the line of code below and then find the normalized value counts for the `Rating` column in `df`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zsEPQgRZKmH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Rating'] = df['Rating'].round().astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCPof-7VZOMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hboaEX03Z_w5",
        "colab_type": "text"
      },
      "source": [
        "What is the baseline accuracy?\n",
        "```\n",
        "Your Answer Here\n",
        "```\n",
        "\n",
        "Visualize the rating counts from above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PGmJSMqZxo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwPg1cpShKNA",
        "colab_type": "text"
      },
      "source": [
        "Use your vectorized tokens in the `df` dataframe to train a classification model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awu-ujvvhips",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGhSLJ5Fhlg9",
        "colab_type": "text"
      },
      "source": [
        "Predict the score of the fake strain description below.\n",
        "\n",
        "```\n",
        "'Afgooey, also known as Afgoo, is a potent indica strain that is believed to descend from an Afghani indica and Maui Haze. \n",
        "Its sativa parent may lend Afgoo some uplifting, creative qualities, but this strain undoubtedly takes after its indica \n",
        "parent as it primarily delivers relaxing, sleepy effects alongside its earthy pine flavor. Growers hoping to cultivate Afgoo \n",
        "may have a better chance of success indoors, but this indica can also thrive in Mediterranean climates outdoors.'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAHaMGjBiG-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGnLTUL8ik4V",
        "colab_type": "text"
      },
      "source": [
        "# Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rfXxSZSDk-Sh"
      },
      "source": [
        "## Questions of Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hlcEfmnyk-St"
      },
      "source": [
        "1. What is Latent Dirichlet Allocation? What is another name for LDA in NLP?\n",
        "```\n",
        "Your Answer Here\n",
        "```\n",
        "\n",
        "2. How do interpret the results of a topic modeling output?\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lAf8cmNFl_n5"
      },
      "source": [
        "## Practice Problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIeP8NyHmAU8",
        "colab_type": "text"
      },
      "source": [
        "Find the top 5 topics of the `Description` column using LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-8zDKA_mAba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADcqbM9FmiVg",
        "colab_type": "text"
      },
      "source": [
        "In a short paragraph, explain how to interpret the first topic your model came up with. If your topic words are difficult to interpret, explain how you could clean up the descriptions to improve your topics\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suchG0sEm8lU",
        "colab_type": "text"
      },
      "source": [
        "Use `pyLDAvis` to create a visualization to help you interpret your topic modeling results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5LbisKnRPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HafoLqwHnR5M",
        "colab_type": "text"
      },
      "source": [
        "Explain how to interpret the results of `pyLDAvis`\n",
        "\n",
        "```\n",
        "Your Answer Here\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANxVUGU2nYsB",
        "colab_type": "text"
      },
      "source": [
        "Create at least 1 more visualization to help you interpret the results of your topic modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEsF_ZMIm7mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}